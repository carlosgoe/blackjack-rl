{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blackjack_game import Blackjack\n",
    "from blackjack_strategy import Strategy\n",
    "from dqn import DQN\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Hide deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get hard and soft tables of basic strategy from file\n",
    "file_path = './strategy/strategy_'\n",
    "basic_strategy = Strategy(src_hard='./strategy/strategy_hard.csv', src_soft='./strategy/strategy_soft.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategy(rl_agent):\n",
    "    # Create empty strategy\n",
    "    strategy = Strategy()\n",
    "    # Create Blackjack environment for card generation\n",
    "    game = Blackjack()\n",
    "    # Iterate over all values for dealer's card (1-10)\n",
    "    for dealers_card in range(1, 11):\n",
    "        # Iterate over values for player's sum (4-20)\n",
    "        for players_sum in range(4, 21):\n",
    "            # Get agent's primary action\n",
    "            observation, invalid_actions = game.reset(players_sum, dealers_card, False)\n",
    "            action = rl_agent.play_one_step(observation, 0, invalid_actions)\n",
    "            # If first choice is 2 (=double) or 3 (=surrender), get alternative action\n",
    "            if action == 2 or action == 3:\n",
    "                observation[-1] = 0\n",
    "                invalid_actions += [2, 3]\n",
    "                alternative = rl_agent.play_one_step(observation, 0, invalid_actions)\n",
    "                # If alternative action is 1 (=hit), table entry is 2 * action\n",
    "                if alternative == 1:\n",
    "                    action *= 2\n",
    "            strategy.hard[players_sum - 4, dealers_card - 1] = action\n",
    "        # Iterate over values for player's sum (12-20)\n",
    "        for players_sum in range(12, 21):\n",
    "            # Get agent's primary action\n",
    "            observation, invalid_actions = game.reset(players_sum, dealers_card, True)\n",
    "            action = rl_agent.play_one_step(observation, 0, invalid_actions)\n",
    "            # If first choice is 2 (=double) or 3 (=surrender), get alternative action\n",
    "            if action == 2 or action == 3:\n",
    "                observation[-1] = 0\n",
    "                invalid_actions += [2, 3]\n",
    "                alternative = rl_agent.play_one_step(observation, 0, invalid_actions)\n",
    "                # If alternative action is 1 (=hit), table entry is 2 * action\n",
    "                if alternative == 1:\n",
    "                    action *= 2\n",
    "            strategy.soft[players_sum - 12, dealers_card - 1] = action\n",
    "    return strategy\n",
    "\n",
    "\n",
    "def test_strategy(strategy, n_games, seed=None, show=False):\n",
    "    # Create new environment and play n games (action values are read from strategy tables) to collect game data\n",
    "    game = Blackjack(seed)\n",
    "    rewards = []\n",
    "    for n in range(n_games):\n",
    "        game.reset()\n",
    "        if show:\n",
    "            print('Game {}/{}:\\n'.format(n + 1, n_games))\n",
    "            game.show()\n",
    "        while not game.done:\n",
    "            # Look up action in correct table (soft or hard)\n",
    "            action = strategy.action(game.players_sum, game.dealer[0], game.usable_ace)\n",
    "            # Case: action is DS or DH\n",
    "            if action == 2 or action == 4:\n",
    "                if not game.first_round:\n",
    "                    action = int(action / 2 == 2)\n",
    "                else:\n",
    "                    action = 2\n",
    "            # Case: action is RS or RH\n",
    "            if action == 3 or action == 6:\n",
    "                if not game.first_round:\n",
    "                    action = int(action / 2 == 3)\n",
    "                else:\n",
    "                    action = 3\n",
    "            game.step(action)\n",
    "            if show:\n",
    "                game.show()\n",
    "        rewards.append(game.reward)\n",
    "        if show:\n",
    "            print()\n",
    "    # Get relative frequency of wins (positive values), losses (negative values), and ties (0s)\n",
    "    s_rewards = np.sign(rewards)\n",
    "    rel_freq = lambda r: np.sum(s_rewards == r) / n_games\n",
    "    data = {'Wins': [rel_freq(1)],\n",
    "            'Losses': [rel_freq(-1)],\n",
    "            'Draws': [rel_freq(0)],\n",
    "            'Mean score': [np.mean(rewards)]}\n",
    "    # Print data frame and return mean score\n",
    "    if not show:\n",
    "        print(pd.DataFrame(data).to_string(index=False))\n",
    "    return data['Mean score'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary parameters for agent\n",
    "n_obs = 30\n",
    "hidden_layers = [(64, 'elu'), (64, 'elu')]\n",
    "n_actions = 4\n",
    "lr_decay = keras.optimizers.schedules.PolynomialDecay(1e-3, 72000, 1e-5)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_decay)\n",
    "discount_factor = 0.99\n",
    "buffer_size = 120000\n",
    "# Best 64, 64 hidden, lr_decay=(1e-3, 72000, 1e-5), optimal_lr=2.5e-4, df=0.99\n",
    "file = None\n",
    "\n",
    "agent = DQN(n_obs, hidden_layers, n_actions, optimizer, discount_factor, buffer_size, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary parameters for session\n",
    "n_episodes = 80000\n",
    "n_pretrain = 20000\n",
    "validation_interval = 500\n",
    "validation_games = 10000\n",
    "validation_seed = random.randint(0, 999)\n",
    "update_target_interval = 4000\n",
    "batch_size = 256\n",
    "epsilon_decay = 0.99, 0.01, 70000\n",
    "\n",
    "# Game is played for e episodes\n",
    "env = Blackjack()\n",
    "mean_scores, matches = [], []\n",
    "best = None\n",
    "for e in range(n_episodes + n_pretrain):\n",
    "    # Get number of training episode\n",
    "    episode = e - n_pretrain\n",
    "    # Calculate epsilon for current episode (epsilon=1 during pretrain)\n",
    "    if episode < 0:\n",
    "        epsilon = 1\n",
    "    else:\n",
    "        epsilon = epsilon_decay[0] - min(episode / epsilon_decay[2], 1) * (epsilon_decay[0] - epsilon_decay[1])\n",
    "    # Get initial state and done value\n",
    "    state, invalid = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Let agent select action (random action with probability epsilon)\n",
    "        agents_action = agent.play_one_step(state, epsilon, invalid)\n",
    "        # Play step and get next state and boolean value of done\n",
    "        next_state, reward, done, invalid = env.step(agents_action)\n",
    "        # Add experience to agent's replay buffer before updating current state\n",
    "        agent.add_experience(state, agents_action, reward, next_state, done, invalid)\n",
    "        state = next_state\n",
    "    if episode >= 0:\n",
    "        # Let agent perform training step\n",
    "        agent.training_step(batch_size)\n",
    "        if episode % update_target_interval == 0 and episode > 0:\n",
    "            agent.update_target_model()\n",
    "        # Get agent's strategy tables and test them\n",
    "        if (episode + 1) % validation_interval == 0:\n",
    "            agents_strategy = get_strategy(agent)\n",
    "            matches.append(agents_strategy.match(basic_strategy))\n",
    "            # Print match percentage compared to basic strategy\n",
    "            output_str = 'Episode {}/{} '.format(episode + 1, n_episodes)\n",
    "            output_str += '- match (basic strategy) = {}%'.format(round(matches[-1] * 100, 1))\n",
    "            print(output_str)\n",
    "            # Get mean score of test and reset seed afterwards\n",
    "            mean_scores.append(test_strategy(agents_strategy, validation_games, validation_seed))\n",
    "            random.seed()\n",
    "            # Save model if it scored best\n",
    "            if best is None or mean_scores[-1] >= best:\n",
    "                agent.save('Blackjack_dqn')\n",
    "                best = mean_scores[-1]\n",
    "            print()\n",
    "\n",
    "# Plot scores\n",
    "f, (ax1, ax2) = plt.subplots(2)\n",
    "ax1.plot(range(len(mean_scores)), mean_scores)\n",
    "ax2.plot(range(len(matches)), matches)\n",
    "ax1.set_ylabel('Mean score')\n",
    "ax2.set_ylabel('Match')\n",
    "ax2.set_xlabel('Episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get agent's strategy\n",
    "agents_strategy = get_strategy(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print strategy tables\n",
    "print('Strategy tables:\\n')\n",
    "agents_strategy.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print match with basic strategy\n",
    "print('Vs. basic strategy: {}%'.format(round(agents_strategy.match(basic_strategy) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test strategy and print result\n",
    "n_of_games = 1000000\n",
    "print('Result ({} games):\\n'.format(n_of_games))\n",
    "test_strategy(agents_strategy, n_of_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Play and show games\n",
    "test_strategy(agents_strategy, 10000, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
